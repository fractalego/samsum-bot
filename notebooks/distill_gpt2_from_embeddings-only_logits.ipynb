{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45879344",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4380648",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings = joblib.load('train_embeddings_logits_only.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98ab1286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['logits_and_indices', 'input_ids'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_embeddings[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898a0fa1",
   "metadata": {},
   "source": [
    "### Training from embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "553939fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import GPT2LMHeadModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5bda74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_small = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13b4e679",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "370453d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_from_summary_and_dialogue(summary, dialogue):\n",
    "    text = f\"\"\"\n",
    "A partial summary of the conversation is:\n",
    "{summary}\n",
    "\n",
    "With the dialogue being:\n",
    "{dialogue}\n",
    "    \"\"\".strip()\n",
    "    \n",
    "    return text.replace('\\r\\n', '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c902d0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = \"\"\"\n",
    "Alberto is a customer. Alberto ordered pizza and lasagne at Dominos one hour ago.\n",
    "Alberto wants to know where his pizza is. Alberto is calling Dominos to know where his orders are.\n",
    "John works at Dominos. John says that his pizza is almost ready. It will be at Alberto's home in 10 minutes. \n",
    "The lasagne is already on its way with the delivery person. Unfortunately the driver had an accident and is now in the hospital.\n",
    "If alberto says \"rabbit\" then John replies \"horse\". If Alberto says \"Tree\" John says \"red\".\n",
    "\"\"\".strip().replace('\\n', ' ')\n",
    "\n",
    "dialogue = \"\"\"\n",
    "Alberto: What happened to my pizza?\n",
    "John: \n",
    "\"\"\".strip()\n",
    "\n",
    "text = create_text_from_summary_and_dialogue(summary, dialogue)\n",
    "\n",
    "prompt = tokenizer(text, return_tensors='pt').to('cuda')\n",
    "prompt = {key: value for key, value in prompt.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f117a243",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "def batchify(data, n):\n",
    "    len_dict = {}\n",
    "    for item in data:\n",
    "        length = item.shape[1]\n",
    "        try:\n",
    "            len_dict[length].append(item)\n",
    "        except:\n",
    "            len_dict[length] = [item]\n",
    "\n",
    "    batch_chunks = []\n",
    "    for k in len_dict.keys():\n",
    "        vectors = len_dict[k]\n",
    "        batch_chunks += chunks(vectors, n)\n",
    "\n",
    "    batches = []\n",
    "    for chunk in batch_chunks:\n",
    "        inputs = torch.stack([item[0] for item in chunk])\n",
    "        batches.append((inputs))\n",
    "\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62d0e120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 0 out of 818\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "val = json.load(open('../data/val.json'))\n",
    "\n",
    "_limit = 1024\n",
    "dev_data = []\n",
    "total_skipped = 0\n",
    "for item in val:\n",
    "    text = create_text_from_summary_and_dialogue(item[\"summary\"], item[\"dialogue\"])\n",
    "    tokens = tokenizer.encode(text, return_tensors='pt')\n",
    "    if tokens.shape[1] > _limit:\n",
    "        tokens = tokens[:, :_limit]\n",
    "    dev_data.append(tokens)\n",
    "    \n",
    "print(f'Skipped {total_skipped} out of {len(val)}')\n",
    "\n",
    "dev_batches = batchify(dev_data, 1)\n",
    "\n",
    "def test(test_model, batches):\n",
    "    test_model.eval()\n",
    "    total_loss = 0.\n",
    "    #for i, batch in tqdm(enumerate(batches), total=len(batches)):\n",
    "    for i, batch in enumerate(batches):\n",
    "        test_model.eval()\n",
    "        inputs = batch\n",
    "        loss = test_model(inputs.cuda(), labels=inputs.cuda())[0]\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1630b8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = gpt_small.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "706886b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev loss: 4.009833007103657\n"
     ]
    }
   ],
   "source": [
    "print('Dev loss:', test(gpt_small, dev_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20b4ee54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probability_vector(log_prob_dict, temp):\n",
    "    _vocab_size = 50257\n",
    "    \n",
    "    logits = torch.tensor(log_prob_dict['logits'])\n",
    "    num_tokens = logits.shape[1]\n",
    "    indices = torch.tensor(log_prob_dict['indices'])\n",
    "    vectors = []\n",
    "    \n",
    "    for index_set, logs in zip(indices[0], logits[0]):\n",
    "        v = torch.sparse_coo_tensor([index_set.tolist()], logs, (_vocab_size, )).to_dense().float()\n",
    "        v[v == 0] = torch.tensor(float('-inf'))\n",
    "        vectors.append(v)\n",
    "\n",
    "    vectors = torch.stack(vectors, dim=0)\n",
    "    return F.softmax(vectors / temp, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b900aef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([56, 50257])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_probability_vector(train_embeddings[0]['logits_and_indices'], temp=10).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "776c1e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█████▏                                | 1998/14732 [03:49<26:16,  8.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|████▉                               | 2001/14732 [03:58<5:35:25,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev loss: 2.431924162546405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██████████▎                           | 3999/14732 [07:36<17:54,  9.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps 4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|█████████▊                          | 4002/14732 [07:44<3:20:20,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev loss: 2.398879644777489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|███████████████▍                      | 5999/14732 [11:29<16:52,  8.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps 6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 41%|██████████████▋                     | 6000/14732 [11:38<6:02:19,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev loss: 2.3750277270897677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|████████████████████▋                 | 7999/14732 [15:11<11:33,  9.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps 8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 54%|███████████████████▌                | 8000/14732 [15:19<4:03:58,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev loss: 2.3613543733407933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|█████████████████████████▊            | 9999/14732 [18:46<08:31,  9.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|███████████████████████▊           | 10002/14732 [18:54<1:50:40,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev loss: 2.3492186436734745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|██████████████████████████████▏      | 11999/14732 [22:21<04:52,  9.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps 12000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████████████████████████▌      | 12001/14732 [22:29<1:20:09,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev loss: 2.340343264612417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|███████████████████████████████████▏ | 13999/14732 [25:56<01:18,  9.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps 14000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|███████████████████████████████████▏ | 14001/14732 [26:04<20:16,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev loss: 2.34194017622465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 14732/14732 [27:19<00:00,  8.98it/s]\n",
      "  9%|███▎                                  | 1267/14732 [02:16<24:38,  9.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps 16000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███                                 | 1269/14732 [02:24<6:31:21,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev loss: 2.3437416345099655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|████████▍                             | 3267/14732 [05:53<20:21,  9.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps 18000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|███████▉                            | 3269/14732 [06:02<5:05:50,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev loss: 2.3482801039819323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|█████████████▌                        | 5267/14732 [09:39<19:42,  8.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps 20000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|████████████▊                       | 5268/14732 [09:48<7:11:29,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev loss: 2.3470166568942643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|██████████████████▋                   | 7266/14732 [13:19<13:06,  9.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps 22000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|█████████████████▊                  | 7269/14732 [13:27<2:56:32,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev loss: 2.350205917813084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|███████████████████████▉              | 9267/14732 [16:56<09:39,  9.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps 24000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 63%|██████████████████████▋             | 9268/14732 [17:05<3:23:26,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev loss: 2.34581764417639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|████████████████████████████▎        | 11267/14732 [20:31<06:10,  9.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps 26000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 76%|██████████████████████████▊        | 11268/14732 [20:39<2:15:57,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev loss: 2.3399405475059174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████████████████████████████▎   | 13267/14732 [24:06<02:32,  9.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps 28000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████████████████████████████▎   | 13270/14732 [24:14<30:53,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev loss: 2.3343154505295973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 14732/14732 [26:45<00:00,  9.18it/s]\n",
      "  4%|█▍                                     | 535/14732 [00:56<25:29,  9.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps 30000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|█▎                                   | 537/14732 [01:04<6:42:28,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev loss: 2.3687014776511996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|██████▌                               | 2534/14732 [04:30<21:03,  9.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps 32000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|██████▏                             | 2536/14732 [04:38<5:40:31,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev loss: 2.426964402927455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███████████▋                          | 4535/14732 [08:16<18:32,  9.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps 34000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███████████                         | 4537/14732 [08:25<4:34:03,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev loss: 2.4412637476816155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|█████████████▎                        | 5139/14732 [09:32<17:47,  8.98it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m loss \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m  temp \u001b[38;5;241m*\u001b[39m temp \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(torch\u001b[38;5;241m.\u001b[39mmul(torch\u001b[38;5;241m.\u001b[39mlog(out_p)\u001b[38;5;241m.\u001b[39mflatten(),\n\u001b[1;32m     25\u001b[0m                                   label_p\u001b[38;5;241m.\u001b[39mflatten()))\n\u001b[1;32m     27\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 29\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     32\u001b[0m steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/src/the_chatbot_experiment/.env/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:65\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     64\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/src/the_chatbot_experiment/.env/lib/python3.8/site-packages/torch/optim/optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/src/the_chatbot_experiment/.env/lib/python3.8/site-packages/torch/autograd/grad_mode.py:28\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m():\n\u001b[0;32m---> 28\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/src/the_chatbot_experiment/.env/lib/python3.8/site-packages/torch/optim/adam.py:107\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[38;5;66;03m# record the step after step update\u001b[39;00m\n\u001b[1;32m    105\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 107\u001b[0m     \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m           \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m           \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m           \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m           \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m           \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m           \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m           \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m           \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m           \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m           \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m           \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/src/the_chatbot_experiment/.env/lib/python3.8/site-packages/torch/optim/_functional.py:94\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     92\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(bias_correction2))\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 94\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbias_correction2\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m     96\u001b[0m step_size \u001b[38;5;241m=\u001b[39m lr \u001b[38;5;241m/\u001b[39m bias_correction1\n\u001b[1;32m     98\u001b[0m param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "lr = 3e-5\n",
    "optimizer = torch.optim.Adam(gpt_small.parameters(), lr=lr)\n",
    "scheduler = StepLR(optimizer, step_size=2, gamma=0.5)\n",
    "epochs = 5\n",
    "\n",
    "steps = 0\n",
    "best_model = None\n",
    "best_loss = 1e6\n",
    "for epoch_num in range(epochs):\n",
    "    gpt_small.train()\n",
    "    temp = 30\n",
    "    random.shuffle(train_embeddings)\n",
    "    \n",
    "    for item in tqdm(train_embeddings):\n",
    "        input_ids = torch.tensor([item['input_ids']]).cuda()\n",
    "        label_p = get_probability_vector(item['logits_and_indices'], temp=temp).cuda()\n",
    "        out_logits = gpt_small.forward(input_ids).logits\n",
    "        out_p = F.softmax(out_logits / temp, dim=-1)\n",
    "        \n",
    "        loss = gpt_small(input_ids, labels=input_ids)[0]\n",
    "        \n",
    "        loss -=  temp * temp * torch.mean(torch.mul(torch.log(out_p).flatten(),\n",
    "                                          label_p.flatten()))\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        steps += 1\n",
    "        \n",
    "        if steps % 2000 == 0:\n",
    "            print(\"steps\", steps)\n",
    "            print('Dev loss:', test(gpt_small, dev_batches))\n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "                best_model = gpt_small\n",
    "            \n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a846218",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Dev loss:', test(best_model, dev_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8a5db6d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [32]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mbest_model\u001b[49m\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt_small_temp30_lr1e-05\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'best_model' is not defined"
     ]
    }
   ],
   "source": [
    "best_model.save_pretrained(f'gpt_small_temp{temp}_lr{lr}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "130da593",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_small = GPT2LMHeadModel.from_pretrained(\"gpt_small_temp30_lr1e-05\")\n",
    "_ = gpt_small.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a47cb8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = gpt_small.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a324ec62",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ab179e39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John: Hello, how can I help?\n",
      "Tree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok, I'm in the office now.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [48]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(dialogue)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m----> 9\u001b[0m     user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     dialogue \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAlberto: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m user_input \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mJohn: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     11\u001b[0m     text \u001b[38;5;241m=\u001b[39m create_text_from_summary_and_dialogue(summary, dialogue)\n",
      "File \u001b[0;32m~/src/the_chatbot_experiment/.env/lib/python3.8/site-packages/ipykernel/kernelbase.py:1044\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allow_stdin:\n\u001b[1;32m   1041\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(\n\u001b[1;32m   1042\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1043\u001b[0m     )\n\u001b[0;32m-> 1044\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/src/the_chatbot_experiment/.env/lib/python3.8/site-packages/ipykernel/kernelbase.py:1089\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1086\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1087\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1088\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m-> 1089\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m   1090\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1091\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "dialogue = \"\"\"\n",
    "John: Hello, how can I help?\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "print(dialogue)\n",
    "\n",
    "while True:\n",
    "    user_input = input()\n",
    "    dialogue += \"\\nAlberto: \" + user_input + \"\\nJohn: \"\n",
    "    text = create_text_from_summary_and_dialogue(summary, dialogue)\n",
    "    prompt = tokenizer(text, return_tensors='pt')\n",
    "    prompt = {key: value.to(device) for key, value in prompt.items()}\n",
    "    out = gpt_small.generate(**prompt, max_length=prompt['input_ids'].shape[1] + 25, do_sample=False)\n",
    "    out = out[0][prompt['input_ids'].shape[1]:]\n",
    "    answer = tokenizer.decode(out)\n",
    "    answer = answer[:answer.find('\\n')].strip()\n",
    "    print(answer)\n",
    "    dialogue += answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1634fa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "summary = \"\"\"\n",
    "Alberto is a customer. Alberto ordered pizza and lasagne at Dominos one hour ago.\n",
    "Alberto wants to know where his pizza is. Alberto is calling Dominos to know where his orders are.\n",
    "John works at Dominos. John says that his pizza is almost ready. It will be at Alberto's home in 10 minutes. \n",
    "The lasagne is already on its way with the delivery person. Unfortunately the driver had an accident and is now in the hospital.\n",
    "If alberto says \"rabbit\" then John replies \"horse\". If Alberto says \"Tree\" John says \"red\".\n",
    "\"\"\".strip().replace('\\n', ' ')\n",
    "\n",
    "dialogue = \"\"\"\n",
    "Alberto: What happened to my pizza?\n",
    "John: It's in the delivery man's car.\n",
    "Alberto: And where is the delivery man?\n",
    "\"\"\".strip()\n",
    "\n",
    "text = create_text_from_summary_and_dialogue(summary, dialogue)\n",
    "\n",
    "prompt = tokenizer(text, return_tensors='pt').to('cuda')\n",
    "prompt = {key: value for key, value in prompt.items()}\n",
    "out = gpt_small.generate(**prompt, max_length=prompt['input_ids'].shape[1] + 10, do_sample=False)\n",
    "print(tokenizer.decode(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08573b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_small.save_pretrained('./gptj_small')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccfbfac",
   "metadata": {},
   "source": [
    "#####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a66f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd17cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = torch.sparse_coo_tensor([[1,2,3]], [4,5,6], (1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c190a99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5136f53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.index_select(v, 0, [1,2,3], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb9a4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb802d97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot3",
   "language": "python",
   "name": "chatbot3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
