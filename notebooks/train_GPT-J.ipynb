{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.cuda.amp import custom_fwd, custom_bwd\n",
    "\n",
    "from bitsandbytes.functional import quantize_blockwise, dequantize_blockwise\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenBNBLinear(nn.Module):\n",
    "    def __init__(self, weight, absmax, code, bias=None):\n",
    "        assert isinstance(bias, nn.Parameter) or bias is None\n",
    "        super().__init__()\n",
    "        self.out_features, self.in_features = weight.shape\n",
    "        self.register_buffer(\"weight\", weight.requires_grad_(False))\n",
    "        self.register_buffer(\"absmax\", absmax.requires_grad_(False))\n",
    "        self.register_buffer(\"code\", code.requires_grad_(False))\n",
    "        self.adapter = None\n",
    "        self.bias = bias\n",
    " \n",
    "    def forward(self, input):\n",
    "        output = DequantizeAndLinear.apply(input, self.weight, self.absmax, self.code, self.bias)\n",
    "        if self.adapter:\n",
    "            output += self.adapter(input)\n",
    "        return output\n",
    " \n",
    "    @classmethod\n",
    "    def from_linear(cls, linear: nn.Linear) -> \"FrozenBNBLinear\":\n",
    "        weights_int8, state = quantize_blockise_lowmemory(linear.weight)\n",
    "        return cls(weights_int8, *state, linear.bias)\n",
    " \n",
    "    def __repr__(self):\n",
    "        return f\"{self.__class__.__name__}({self.in_features}, {self.out_features})\"\n",
    " \n",
    " \n",
    "class DequantizeAndLinear(torch.autograd.Function): \n",
    "    @staticmethod\n",
    "    @custom_fwd\n",
    "    def forward(ctx, input: torch.Tensor, weights_quantized: torch.ByteTensor,\n",
    "                absmax: torch.FloatTensor, code: torch.FloatTensor, bias: torch.FloatTensor):\n",
    "        weights_deq = dequantize_blockwise(weights_quantized, absmax=absmax, code=code)\n",
    "        ctx.save_for_backward(input, weights_quantized, absmax, code)\n",
    "        ctx._has_bias = bias is not None\n",
    "        return F.linear(input, weights_deq, bias)\n",
    " \n",
    "    @staticmethod\n",
    "    @custom_bwd\n",
    "    def backward(ctx, grad_output: torch.Tensor):\n",
    "        assert not ctx.needs_input_grad[1] and not ctx.needs_input_grad[2] and not ctx.needs_input_grad[3]\n",
    "        input, weights_quantized, absmax, code = ctx.saved_tensors\n",
    "        # grad_output: [*batch, out_features]\n",
    "        weights_deq = dequantize_blockwise(weights_quantized, absmax=absmax, code=code)\n",
    "        grad_input = grad_output @ weights_deq\n",
    "        grad_bias = grad_output.flatten(0, -2).sum(dim=0) if ctx._has_bias else None\n",
    "        return grad_input, None, None, None, grad_bias\n",
    " \n",
    " \n",
    "class FrozenBNBEmbedding(nn.Module):\n",
    "    def __init__(self, weight, absmax, code):\n",
    "        super().__init__()\n",
    "        self.num_embeddings, self.embedding_dim = weight.shape\n",
    "        self.register_buffer(\"weight\", weight.requires_grad_(False))\n",
    "        self.register_buffer(\"absmax\", absmax.requires_grad_(False))\n",
    "        self.register_buffer(\"code\", code.requires_grad_(False))\n",
    "        self.adapter = None\n",
    " \n",
    "    def forward(self, input, **kwargs):\n",
    "        with torch.no_grad():\n",
    "            # note: both quantuized weights and input indices are *not* differentiable\n",
    "            weight_deq = dequantize_blockwise(self.weight, absmax=self.absmax, code=self.code)\n",
    "            output = F.embedding(input, weight_deq, **kwargs)\n",
    "        if self.adapter:\n",
    "            output += self.adapter(input)\n",
    "        return output \n",
    " \n",
    "    @classmethod\n",
    "    def from_embedding(cls, embedding: nn.Embedding) -> \"FrozenBNBEmbedding\":\n",
    "        weights_int8, state = quantize_blockise_lowmemory(embedding.weight)\n",
    "        return cls(weights_int8, *state)\n",
    " \n",
    "    def __repr__(self):\n",
    "        return f\"{self.__class__.__name__}({self.num_embeddings}, {self.embedding_dim})\"\n",
    " \n",
    " \n",
    "def quantize_blockise_lowmemory(matrix: torch.Tensor, chunk_size: int = 2 ** 20):\n",
    "    assert chunk_size % 4096 == 0\n",
    "    code = None\n",
    "    chunks = []\n",
    "    absmaxes = []\n",
    "    flat_tensor = matrix.view(-1)\n",
    "    for i in range((matrix.numel() - 1) // chunk_size + 1):\n",
    "        input_chunk = flat_tensor[i * chunk_size: (i + 1) * chunk_size].clone()\n",
    "        quantized_chunk, (absmax_chunk, code) = quantize_blockwise(input_chunk, code=code)\n",
    "        chunks.append(quantized_chunk)\n",
    "        absmaxes.append(absmax_chunk)\n",
    " \n",
    "    matrix_i8 = torch.cat(chunks).reshape_as(matrix)\n",
    "    absmax = torch.cat(absmaxes)\n",
    "    return matrix_i8, (absmax, code)\n",
    " \n",
    " \n",
    "def convert_to_int8(model):\n",
    "    \"\"\"Convert linear and embedding modules to 8-bit with optional adapters\"\"\"\n",
    "    for module in list(model.modules()):\n",
    "        for name, child in module.named_children():\n",
    "            if isinstance(child, nn.Linear):\n",
    "                print(name, child)\n",
    "                setattr( \n",
    "                    module,\n",
    "                    name,\n",
    "                    FrozenBNBLinear(\n",
    "                        weight=torch.zeros(child.out_features, child.in_features, dtype=torch.uint8),\n",
    "                        absmax=torch.zeros((child.weight.numel() - 1) // 4096 + 1),\n",
    "                        code=torch.zeros(256),\n",
    "                        bias=child.bias,\n",
    "                    ),\n",
    "                )\n",
    "            elif isinstance(child, nn.Embedding):\n",
    "                setattr(\n",
    "                    module,\n",
    "                    name,\n",
    "                    FrozenBNBEmbedding(\n",
    "                        weight=torch.zeros(child.num_embeddings, child.embedding_dim, dtype=torch.uint8),\n",
    "                        absmax=torch.zeros((child.weight.numel() - 1) // 4096 + 1),\n",
    "                        code=torch.zeros(256),\n",
    "                    )\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTJBlock(transformers.models.gptj.modeling_gptj.GPTJBlock):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        convert_to_int8(self.attn)\n",
    "        convert_to_int8(self.mlp)\n",
    "\n",
    "\n",
    "class GPTJModel(transformers.models.gptj.modeling_gptj.GPTJModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        convert_to_int8(self)\n",
    "        \n",
    "\n",
    "class GPTJForCausalLM(transformers.models.gptj.modeling_gptj.GPTJForCausalLM):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        convert_to_int8(self)\n",
    "\n",
    "\n",
    "transformers.models.gptj.modeling_gptj.GPTJBlock = GPTJBlock  # monkey-patch GPT-J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = transformers.GPTJConfig.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTJConfig {\n",
       "  \"activation_function\": \"gelu_new\",\n",
       "  \"architectures\": [\n",
       "    \"GPTJForCausalLM\"\n",
       "  ],\n",
       "  \"attn_pdrop\": 0.0,\n",
       "  \"bos_token_id\": 50256,\n",
       "  \"embd_pdrop\": 0.0,\n",
       "  \"eos_token_id\": 50256,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"model_type\": \"gptj\",\n",
       "  \"n_embd\": 4096,\n",
       "  \"n_head\": 16,\n",
       "  \"n_inner\": null,\n",
       "  \"n_layer\": 28,\n",
       "  \"n_positions\": 2048,\n",
       "  \"resid_pdrop\": 0.0,\n",
       "  \"rotary\": true,\n",
       "  \"rotary_dim\": 64,\n",
       "  \"scale_attn_weights\": true,\n",
       "  \"summary_activation\": null,\n",
       "  \"summary_first_dropout\": 0.1,\n",
       "  \"summary_proj_to_labels\": true,\n",
       "  \"summary_type\": \"cls_index\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"task_specific_params\": {\n",
       "    \"text-generation\": {\n",
       "      \"do_sample\": true,\n",
       "      \"max_length\": 50,\n",
       "      \"temperature\": 1.0\n",
       "    }\n",
       "  },\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"tokenizer_class\": \"GPT2Tokenizer\",\n",
       "  \"transformers_version\": \"4.14.1\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50400\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "lm_head Linear(in_features=4096, out_features=50400, bias=True)\n"
     ]
    }
   ],
   "source": [
    "gpt = GPTJForCausalLM.from_pretrained(\"hivemind/gpt-j-6B-8bit\", low_cpu_mem_usage=True)\n",
    "_ = gpt.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gpt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[43mgpt\u001b[49m\u001b[38;5;241m.\u001b[39mparameters()):\n\u001b[1;32m      2\u001b[0m     p\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gpt' is not defined"
     ]
    }
   ],
   "source": [
    "for p in list(gpt.parameters()):\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_params(model):\n",
    "    pp=0\n",
    "    for p in list(model.parameters()):\n",
    "        if not p.requires_grad:\n",
    "            continue\n",
    "\n",
    "        nn=1\n",
    "        for s in list(p.size()):\n",
    "            nn = nn*s\n",
    "        pp += nn\n",
    "    return pp\n",
    "\n",
    "get_n_params(gpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gpt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43mgpt\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gpt' is not defined"
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "_ = gpt.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gpt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:12\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gpt' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "text = f\"\"\"\n",
    "A garbled conversation is transcribed as follows:\n",
    "Left: THIS IS MIKE FROM QB EINSURANCE HOW CAN I HELP YOU\n",
    "Right: NOT TO BAD NOT TO BAD HOW ARE YOU\n",
    "Left: ALL IS GOOD DO YOU HAVE A POLICY NUMBER\n",
    "\n",
    "Please correct the conversation and write it below, in lower case and adding punctuation:\n",
    "\"\"\".strip()\n",
    "\n",
    "prompt = tokenizer(text, return_tensors='pt')\n",
    "prompt = {key: value.to(device) for key, value in prompt.items()}\n",
    "out = gpt.generate(**prompt, max_length=prompt['input_ids'].shape[1] + 50, do_sample=False)\n",
    "print(tokenizer.decode(out[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "train = json.load(open('../data/train.json'))\n",
    "val = json.load(open('../data/val.json'))\n",
    "test = json.load(open('../data/test.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTJForCausalLM(\n",
       "  (transformer): GPTJModel(\n",
       "    (wte): FrozenBNBEmbedding(50400, 4096)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (12): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (13): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (14): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (15): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (16): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (17): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (18): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (19): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (20): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (21): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (22): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (23): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (24): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (25): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (26): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (27): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): FrozenBNBLinear(4096, 50400)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_adapters(model, adapter_dim=16):\n",
    "    assert adapter_dim > 0\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, FrozenBNBLinear):\n",
    "            if not 'attn' in name:\n",
    "                continue\n",
    "\n",
    "            module.adapter = nn.Sequential(\n",
    "                nn.Linear(module.in_features, adapter_dim, bias=False),\n",
    "                nn.Linear(adapter_dim, module.out_features, bias=False),\n",
    "            )\n",
    "            nn.init.zeros_(module.adapter[1].weight)\n",
    "        elif isinstance(module, FrozenBNBEmbedding):\n",
    "            module.adapter = nn.Sequential(\n",
    "                nn.Embedding(module.num_embeddings, adapter_dim),\n",
    "                nn.Linear(adapter_dim, module.embedding_dim, bias=False),\n",
    "            )\n",
    "            nn.init.zeros_(module.adapter[1].weight)\n",
    "\n",
    "add_adapters(gpt)\n",
    "gpt.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_n_params(gpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_for_optimizer = [\n",
    "    param for name, param in gpt.named_parameters()\n",
    "    if \"attn\" in name and \"adapter\" in name\n",
    "]\n",
    "print(\"Trainiable params:\", len(params_for_optimizer))\n",
    "\n",
    "for name, param in gpt.named_parameters():\n",
    "    if \"attn\" in name and \"adapter\" in name:\n",
    "        continue\n",
    "\n",
    "    print(f\"Setting {name} requires_grad=False\")\n",
    "    param.requires_grad = False\n",
    "\n",
    "get_n_params(gpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "def batchify(data, n):\n",
    "    len_dict = {}\n",
    "    for item in data:\n",
    "        length = item.shape[1]\n",
    "        try:\n",
    "            len_dict[length].append(item)\n",
    "        except:\n",
    "            len_dict[length] = [item]\n",
    "\n",
    "    batch_chunks = []\n",
    "    for k in len_dict.keys():\n",
    "        vectors = len_dict[k]\n",
    "        batch_chunks += chunks(vectors, n)\n",
    "\n",
    "    batches = []\n",
    "    for chunk in batch_chunks:\n",
    "        inputs = torch.stack([item[0] for item in chunk])\n",
    "        batches.append((inputs))\n",
    "\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_from_summary_and_dialogue(summary, dialogue):\n",
    "    text = f\"\"\"\n",
    "A partial summary of the conversation is:\n",
    "{summary}\n",
    "\n",
    "With the dialogue being:\n",
    "{dialogue}\n",
    "    \"\"\".strip()\n",
    "    \n",
    "    return text.replace('\\r\\n', '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 0 out of 14732\n"
     ]
    }
   ],
   "source": [
    "_limit = 1024\n",
    "train_data = []\n",
    "total_skipped = 0\n",
    "for item in train:\n",
    "    text = create_text_from_summary_and_dialogue(item[\"summary\"], item[\"dialogue\"])\n",
    "    tokens = tokenizer.encode(text, return_tensors='pt', truncation=True, max_length=110)\n",
    "    if tokens.shape[1] > _limit:\n",
    "        tokens = tokens[:, :_limit]\n",
    "    train_data.append(tokens)\n",
    "    \n",
    "print(f'Skipped {total_skipped} out of {len(train)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 0 out of 818\n"
     ]
    }
   ],
   "source": [
    "_limit = 1024\n",
    "dev_data = []\n",
    "total_skipped = 0\n",
    "for item in val:\n",
    "    text = create_text_from_summary_and_dialogue(item[\"summary\"], item[\"dialogue\"])\n",
    "    tokens = tokenizer.encode(text, return_tensors='pt', truncation=True, max_length=110)\n",
    "    if tokens.shape[1] > _limit:\n",
    "        tokens = tokens[:, :_limit]\n",
    "    dev_data.append(tokens)\n",
    "    \n",
    "print(f'Skipped {total_skipped} out of {len(val)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_model, batches, optimizer, criterion):\n",
    "    train_model.train()\n",
    "    total_loss = 0.\n",
    "    for i, batch in tqdm(enumerate(batches), total=len(batches)):\n",
    "        model.train()\n",
    "        inputs = batch\n",
    "        optimizer.zero_grad()\n",
    "        loss = train_model(inputs.cuda(), labels=inputs.cuda())[0]\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(train_model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(batches)\n",
    "\n",
    "def test(test_model, batches):\n",
    "    test_model.eval()\n",
    "    total_loss = 0.\n",
    "    for i, batch in tqdm(enumerate(batches), total=len(batches)):\n",
    "        test_model.eval()\n",
    "        inputs = batch\n",
    "        loss = test_model(inputs.cuda(), labels=inputs.cuda())[0]\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = batchify(train_data, 1)\n",
    "#dev_batches = batchify(dev_data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aafaad436e9248aca594bbd0f2dd1d08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14732 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from bitsandbytes.optim import Adam8bit\n",
    "\n",
    "gpt.gradient_checkpointing_enable()\n",
    "\n",
    "optimizer = Adam8bit(gpt.parameters(), lr=1e-6)\n",
    "\n",
    "with torch.cuda.amp.autocast():\n",
    "    for batch in tqdm(train_batches):\n",
    "        out = gpt.forward(batch.cuda())\n",
    "\n",
    "        loss = F.cross_entropy(out.logits[:, :-1, :].flatten(0, -2), batch[:, 1:].flatten().cuda(),\n",
    "                               reduction='mean')\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        del batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt.save_pretrained('./gpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 10.76 GiB total capacity; 9.37 GiB already allocated; 41.19 MiB free; 9.37 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m gpt\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./gpt/pytorch_model.bin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/src/the_chatbot_experiment/.env/lib/python3.8/site-packages/torch/serialization.py:607\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    605\u001b[0m             opened_file\u001b[38;5;241m.\u001b[39mseek(orig_position)\n\u001b[1;32m    606\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mload(opened_file)\n\u001b[0;32m--> 607\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n",
      "File \u001b[0;32m~/src/the_chatbot_experiment/.env/lib/python3.8/site-packages/torch/serialization.py:882\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m    880\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m    881\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m--> 882\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    884\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m    886\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/src/the_chatbot_experiment/.env/lib/python3.8/site-packages/torch/serialization.py:857\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m    855\u001b[0m data_type, key, location, size \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m    856\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m loaded_storages:\n\u001b[0;32m--> 857\u001b[0m     \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    858\u001b[0m storage \u001b[38;5;241m=\u001b[39m loaded_storages[key]\n\u001b[1;32m    859\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m storage\n",
      "File \u001b[0;32m~/src/the_chatbot_experiment/.env/lib/python3.8/site-packages/torch/serialization.py:846\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(data_type, size, key, location)\u001b[0m\n\u001b[1;32m    843\u001b[0m dtype \u001b[38;5;241m=\u001b[39m data_type(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    845\u001b[0m storage \u001b[38;5;241m=\u001b[39m zip_file\u001b[38;5;241m.\u001b[39mget_storage_from_record(name, size, dtype)\u001b[38;5;241m.\u001b[39mstorage()\n\u001b[0;32m--> 846\u001b[0m loaded_storages[key] \u001b[38;5;241m=\u001b[39m \u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/src/the_chatbot_experiment/.env/lib/python3.8/site-packages/torch/serialization.py:175\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 175\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/src/the_chatbot_experiment/.env/lib/python3.8/site-packages/torch/serialization.py:157\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m storage_type(obj\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/src/the_chatbot_experiment/.env/lib/python3.8/site-packages/torch/_utils.py:79\u001b[0m, in \u001b[0;36m_cuda\u001b[0;34m(self, device, non_blocking, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     new_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcuda, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m---> 79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnew_type\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcopy_(\u001b[38;5;28mself\u001b[39m, non_blocking)\n",
      "File \u001b[0;32m~/src/the_chatbot_experiment/.env/lib/python3.8/site-packages/torch/cuda/__init__.py:528\u001b[0m, in \u001b[0;36m_lazy_new\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    525\u001b[0m _lazy_init()\n\u001b[1;32m    526\u001b[0m \u001b[38;5;66;03m# We may need to call lazy init again if we are a forked child\u001b[39;00m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;66;03m# del _CudaBase.__new__\u001b[39;00m\n\u001b[0;32m--> 528\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_CudaBase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__new__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 10.76 GiB total capacity; 9.37 GiB already allocated; 41.19 MiB free; 9.37 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "gpt.load_state_dict(torch.load('./gpt/pytorch_model.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "lm_head Linear(in_features=4096, out_features=50400, bias=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./gpt were not used when initializing GPTJForCausalLM: ['transformer.h.5.attn.v_proj.adapter.0.weight', 'transformer.h.21.attn.q_proj.adapter.1.weight', 'transformer.wte.adapter.0.weight', 'transformer.h.8.attn.v_proj.adapter.1.weight', 'transformer.h.23.attn.out_proj.adapter.1.weight', 'transformer.h.17.attn.v_proj.adapter.0.weight', 'transformer.h.21.attn.out_proj.adapter.0.weight', 'transformer.h.16.attn.q_proj.adapter.1.weight', 'transformer.h.13.attn.k_proj.adapter.1.weight', 'transformer.h.20.attn.q_proj.adapter.1.weight', 'transformer.h.27.attn.k_proj.adapter.0.weight', 'transformer.h.10.attn.q_proj.adapter.1.weight', 'transformer.h.14.attn.out_proj.adapter.1.weight', 'transformer.h.3.attn.out_proj.adapter.0.weight', 'transformer.h.16.attn.k_proj.adapter.0.weight', 'transformer.h.3.attn.q_proj.adapter.0.weight', 'transformer.h.17.attn.out_proj.adapter.1.weight', 'transformer.h.0.attn.out_proj.adapter.1.weight', 'transformer.h.1.attn.q_proj.adapter.1.weight', 'transformer.h.27.attn.out_proj.adapter.0.weight', 'transformer.h.14.attn.k_proj.adapter.1.weight', 'transformer.h.6.attn.v_proj.adapter.1.weight', 'transformer.h.12.attn.k_proj.adapter.1.weight', 'transformer.h.22.attn.out_proj.adapter.0.weight', 'transformer.h.9.attn.q_proj.adapter.0.weight', 'transformer.h.25.attn.q_proj.adapter.1.weight', 'transformer.h.6.attn.out_proj.adapter.1.weight', 'transformer.h.18.attn.k_proj.adapter.0.weight', 'transformer.h.13.attn.out_proj.adapter.0.weight', 'transformer.h.6.attn.v_proj.adapter.0.weight', 'transformer.h.1.attn.q_proj.adapter.0.weight', 'transformer.h.10.attn.out_proj.adapter.0.weight', 'transformer.h.10.attn.v_proj.adapter.1.weight', 'transformer.h.9.attn.out_proj.adapter.0.weight', 'transformer.h.16.attn.out_proj.adapter.1.weight', 'transformer.h.17.attn.q_proj.adapter.1.weight', 'transformer.h.21.attn.v_proj.adapter.1.weight', 'transformer.h.3.attn.v_proj.adapter.1.weight', 'transformer.h.0.attn.v_proj.adapter.0.weight', 'transformer.h.26.attn.v_proj.adapter.0.weight', 'transformer.h.1.attn.k_proj.adapter.1.weight', 'transformer.h.15.attn.v_proj.adapter.0.weight', 'transformer.h.20.attn.q_proj.adapter.0.weight', 'transformer.h.15.attn.k_proj.adapter.0.weight', 'transformer.h.24.attn.out_proj.adapter.1.weight', 'transformer.h.26.attn.k_proj.adapter.1.weight', 'transformer.h.26.attn.v_proj.adapter.1.weight', 'transformer.h.11.attn.v_proj.adapter.0.weight', 'transformer.h.4.attn.q_proj.adapter.1.weight', 'transformer.h.7.attn.out_proj.adapter.1.weight', 'transformer.h.19.attn.v_proj.adapter.0.weight', 'transformer.h.2.attn.q_proj.adapter.1.weight', 'transformer.h.23.attn.v_proj.adapter.1.weight', 'transformer.h.11.attn.k_proj.adapter.1.weight', 'transformer.h.3.attn.out_proj.adapter.1.weight', 'transformer.h.26.attn.k_proj.adapter.0.weight', 'transformer.h.22.attn.k_proj.adapter.1.weight', 'transformer.h.2.attn.out_proj.adapter.1.weight', 'transformer.h.19.attn.k_proj.adapter.0.weight', 'transformer.h.21.attn.k_proj.adapter.0.weight', 'transformer.h.1.attn.v_proj.adapter.1.weight', 'transformer.h.7.attn.k_proj.adapter.0.weight', 'transformer.h.23.attn.k_proj.adapter.0.weight', 'transformer.h.12.attn.q_proj.adapter.0.weight', 'transformer.h.21.attn.k_proj.adapter.1.weight', 'transformer.h.21.attn.q_proj.adapter.0.weight', 'transformer.h.2.attn.out_proj.adapter.0.weight', 'transformer.h.18.attn.v_proj.adapter.1.weight', 'transformer.h.4.attn.k_proj.adapter.1.weight', 'transformer.h.2.attn.k_proj.adapter.0.weight', 'transformer.h.3.attn.k_proj.adapter.0.weight', 'transformer.h.18.attn.v_proj.adapter.0.weight', 'transformer.h.18.attn.q_proj.adapter.1.weight', 'transformer.h.7.attn.k_proj.adapter.1.weight', 'transformer.h.17.attn.out_proj.adapter.0.weight', 'transformer.h.20.attn.v_proj.adapter.1.weight', 'transformer.h.25.attn.k_proj.adapter.1.weight', 'transformer.h.16.attn.v_proj.adapter.1.weight', 'transformer.h.4.attn.out_proj.adapter.0.weight', 'transformer.h.23.attn.v_proj.adapter.0.weight', 'transformer.h.17.attn.v_proj.adapter.1.weight', 'transformer.h.9.attn.v_proj.adapter.0.weight', 'transformer.h.13.attn.q_proj.adapter.1.weight', 'transformer.h.19.attn.k_proj.adapter.1.weight', 'transformer.h.0.attn.v_proj.adapter.1.weight', 'transformer.h.7.attn.q_proj.adapter.0.weight', 'transformer.h.8.attn.v_proj.adapter.0.weight', 'transformer.h.12.attn.out_proj.adapter.0.weight', 'transformer.h.21.attn.out_proj.adapter.1.weight', 'transformer.h.22.attn.v_proj.adapter.1.weight', 'transformer.h.10.attn.v_proj.adapter.0.weight', 'transformer.h.2.attn.v_proj.adapter.0.weight', 'transformer.h.8.attn.k_proj.adapter.1.weight', 'transformer.h.11.attn.k_proj.adapter.0.weight', 'transformer.h.27.attn.q_proj.adapter.1.weight', 'transformer.h.17.attn.k_proj.adapter.1.weight', 'transformer.h.22.attn.k_proj.adapter.0.weight', 'transformer.h.25.attn.out_proj.adapter.1.weight', 'transformer.h.15.attn.out_proj.adapter.1.weight', 'transformer.h.5.attn.k_proj.adapter.1.weight', 'transformer.h.7.attn.v_proj.adapter.0.weight', 'transformer.h.11.attn.out_proj.adapter.1.weight', 'transformer.h.15.attn.q_proj.adapter.0.weight', 'transformer.h.27.attn.q_proj.adapter.0.weight', 'transformer.h.14.attn.q_proj.adapter.1.weight', 'transformer.h.9.attn.v_proj.adapter.1.weight', 'transformer.h.20.attn.k_proj.adapter.0.weight', 'transformer.h.23.attn.q_proj.adapter.1.weight', 'transformer.h.1.attn.out_proj.adapter.1.weight', 'transformer.h.4.attn.v_proj.adapter.0.weight', 'transformer.h.2.attn.v_proj.adapter.1.weight', 'transformer.h.24.attn.q_proj.adapter.0.weight', 'transformer.h.10.attn.k_proj.adapter.1.weight', 'transformer.h.10.attn.out_proj.adapter.1.weight', 'transformer.h.17.attn.q_proj.adapter.0.weight', 'transformer.h.8.attn.q_proj.adapter.0.weight', 'transformer.h.24.attn.k_proj.adapter.1.weight', 'transformer.h.19.attn.out_proj.adapter.0.weight', 'transformer.h.3.attn.k_proj.adapter.1.weight', 'transformer.h.23.attn.out_proj.adapter.0.weight', 'transformer.h.18.attn.k_proj.adapter.1.weight', 'transformer.h.25.attn.v_proj.adapter.1.weight', 'transformer.h.7.attn.q_proj.adapter.1.weight', 'transformer.h.0.attn.q_proj.adapter.1.weight', 'transformer.h.3.attn.v_proj.adapter.0.weight', 'transformer.h.22.attn.q_proj.adapter.0.weight', 'transformer.h.6.attn.k_proj.adapter.0.weight', 'transformer.h.24.attn.q_proj.adapter.1.weight', 'transformer.h.25.attn.q_proj.adapter.0.weight', 'transformer.h.4.attn.v_proj.adapter.1.weight', 'transformer.h.11.attn.q_proj.adapter.1.weight', 'transformer.wte.adapter.1.weight', 'transformer.h.17.attn.k_proj.adapter.0.weight', 'transformer.h.12.attn.out_proj.adapter.1.weight', 'transformer.h.13.attn.v_proj.adapter.1.weight', 'transformer.h.14.attn.v_proj.adapter.0.weight', 'transformer.h.4.attn.q_proj.adapter.0.weight', 'transformer.h.14.attn.k_proj.adapter.0.weight', 'transformer.h.19.attn.v_proj.adapter.1.weight', 'transformer.h.25.attn.v_proj.adapter.0.weight', 'transformer.h.24.attn.out_proj.adapter.0.weight', 'transformer.h.14.attn.v_proj.adapter.1.weight', 'transformer.h.11.attn.q_proj.adapter.0.weight', 'transformer.h.15.attn.q_proj.adapter.1.weight', 'transformer.h.27.attn.v_proj.adapter.0.weight', 'transformer.h.5.attn.k_proj.adapter.0.weight', 'transformer.h.0.attn.q_proj.adapter.0.weight', 'transformer.h.1.attn.k_proj.adapter.0.weight', 'transformer.h.6.attn.k_proj.adapter.1.weight', 'transformer.h.10.attn.k_proj.adapter.0.weight', 'transformer.h.5.attn.out_proj.adapter.0.weight', 'transformer.h.2.attn.q_proj.adapter.0.weight', 'transformer.h.4.attn.k_proj.adapter.0.weight', 'transformer.h.26.attn.out_proj.adapter.0.weight', 'transformer.h.6.attn.q_proj.adapter.1.weight', 'transformer.h.26.attn.q_proj.adapter.1.weight', 'transformer.h.0.attn.k_proj.adapter.0.weight', 'transformer.h.22.attn.v_proj.adapter.0.weight', 'transformer.h.23.attn.k_proj.adapter.1.weight', 'transformer.h.13.attn.k_proj.adapter.0.weight', 'transformer.h.18.attn.q_proj.adapter.0.weight', 'transformer.h.8.attn.out_proj.adapter.0.weight', 'transformer.h.5.attn.q_proj.adapter.1.weight', 'transformer.h.27.attn.out_proj.adapter.1.weight', 'transformer.h.16.attn.q_proj.adapter.0.weight', 'transformer.h.0.attn.k_proj.adapter.1.weight', 'transformer.h.16.attn.k_proj.adapter.1.weight', 'transformer.h.20.attn.out_proj.adapter.0.weight', 'transformer.h.15.attn.out_proj.adapter.0.weight', 'transformer.h.1.attn.out_proj.adapter.0.weight', 'transformer.h.21.attn.v_proj.adapter.0.weight', 'transformer.h.25.attn.k_proj.adapter.0.weight', 'transformer.h.6.attn.out_proj.adapter.0.weight', 'transformer.h.12.attn.k_proj.adapter.0.weight', 'transformer.h.27.attn.v_proj.adapter.1.weight', 'transformer.h.9.attn.k_proj.adapter.1.weight', 'transformer.h.19.attn.q_proj.adapter.1.weight', 'transformer.h.12.attn.v_proj.adapter.1.weight', 'transformer.h.20.attn.k_proj.adapter.1.weight', 'transformer.h.5.attn.q_proj.adapter.0.weight', 'transformer.h.5.attn.out_proj.adapter.1.weight', 'transformer.h.26.attn.q_proj.adapter.0.weight', 'transformer.h.22.attn.out_proj.adapter.1.weight', 'transformer.h.13.attn.out_proj.adapter.1.weight', 'transformer.h.16.attn.out_proj.adapter.0.weight', 'transformer.h.23.attn.q_proj.adapter.0.weight', 'transformer.h.11.attn.out_proj.adapter.0.weight', 'transformer.h.8.attn.out_proj.adapter.1.weight', 'transformer.h.26.attn.out_proj.adapter.1.weight', 'transformer.h.13.attn.v_proj.adapter.0.weight', 'transformer.h.18.attn.out_proj.adapter.1.weight', 'transformer.h.9.attn.out_proj.adapter.1.weight', 'transformer.h.9.attn.k_proj.adapter.0.weight', 'transformer.h.24.attn.v_proj.adapter.0.weight', 'transformer.h.20.attn.v_proj.adapter.0.weight', 'transformer.h.1.attn.v_proj.adapter.0.weight', 'transformer.h.11.attn.v_proj.adapter.1.weight', 'transformer.h.20.attn.out_proj.adapter.1.weight', 'transformer.h.14.attn.out_proj.adapter.0.weight', 'transformer.h.7.attn.out_proj.adapter.0.weight', 'transformer.h.3.attn.q_proj.adapter.1.weight', 'transformer.h.24.attn.k_proj.adapter.0.weight', 'transformer.h.24.attn.v_proj.adapter.1.weight', 'transformer.h.4.attn.out_proj.adapter.1.weight', 'transformer.h.19.attn.q_proj.adapter.0.weight', 'transformer.h.7.attn.v_proj.adapter.1.weight', 'transformer.h.27.attn.k_proj.adapter.1.weight', 'transformer.h.8.attn.q_proj.adapter.1.weight', 'transformer.h.0.attn.out_proj.adapter.0.weight', 'transformer.h.12.attn.v_proj.adapter.0.weight', 'transformer.h.8.attn.k_proj.adapter.0.weight', 'transformer.h.12.attn.q_proj.adapter.1.weight', 'transformer.h.25.attn.out_proj.adapter.0.weight', 'transformer.h.16.attn.v_proj.adapter.0.weight', 'transformer.h.18.attn.out_proj.adapter.0.weight', 'transformer.h.2.attn.k_proj.adapter.1.weight', 'transformer.h.19.attn.out_proj.adapter.1.weight', 'transformer.h.15.attn.k_proj.adapter.1.weight', 'transformer.h.10.attn.q_proj.adapter.0.weight', 'transformer.h.15.attn.v_proj.adapter.1.weight', 'transformer.h.5.attn.v_proj.adapter.1.weight', 'transformer.h.13.attn.q_proj.adapter.0.weight', 'transformer.h.9.attn.q_proj.adapter.1.weight', 'transformer.h.14.attn.q_proj.adapter.0.weight', 'transformer.h.6.attn.q_proj.adapter.0.weight', 'transformer.h.22.attn.q_proj.adapter.1.weight']\n",
      "- This IS expected if you are initializing GPTJForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPTJForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m gpt \u001b[38;5;241m=\u001b[39m GPTJForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./gpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m _ \u001b[38;5;241m=\u001b[39m gpt\u001b[38;5;241m.\u001b[39mto(\u001b[43mdevice\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "gpt = GPTJForCausalLM.from_pretrained('./gpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = gpt.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_text_from_summary_and_dialogue' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m summary \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124mAlberto is a customer. Alberto ordered pizza and lasagne at Dominos one hour ago.\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124mAlberto wants to know where his pizza is. Alberto is calling Dominos to know where his orders are.\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124mIf alberto says \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrabbit\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m then John replies \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhorse\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. If Alberto says \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTree\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m John says \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m dialogue \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124mJohn: Hello, how can I help?\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m---> 13\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_text_from_summary_and_dialogue\u001b[49m(summary, dialogue)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'create_text_from_summary_and_dialogue' is not defined"
     ]
    }
   ],
   "source": [
    "summary = \"\"\"\n",
    "Alberto is a customer. Alberto ordered pizza and lasagne at Dominos one hour ago.\n",
    "Alberto wants to know where his pizza is. Alberto is calling Dominos to know where his orders are.\n",
    "John works at Dominos. John says that his pizza is almost ready. It will be at Alberto's home in 10 minutes. \n",
    "The lasagne is already on its way with the delivery person. Unfortunately the driver had an accident and is now in the hospital.\n",
    "If alberto says \"rabbit\" then John replies \"horse\". If Alberto says \"Tree\" John says \"red\".\n",
    "\"\"\".strip().replace('\\n', ' ')\n",
    "\n",
    "dialogue = \"\"\"\n",
    "John: Hello, how can I help?\n",
    "\"\"\".strip()\n",
    "\n",
    "text = create_text_from_summary_and_dialogue(summary, dialogue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A partial summary of the conversation is:\n",
      "Alberto is a customer. Alberto ordered pizza and lasagne at Dominos one hour ago. Alberto wants to know where his pizza is. Alberto is calling Dominos to know where his orders are. John works at Dominos. John says that his pizza is almost ready. It will be at Alberto's home in 10 minutes.  The lasagne is already on its way with the delivery person. Unfortunately the driver had an accident and is now in the hospital.\n",
      "\n",
      "With the dialogue being:\n",
      "Alberto: Hello, where are my orders?\n",
      "John: Your pizza is almost ready. It will be at your home in 10 minutes.\n",
      "Alberto: what about the lasagne?\n",
      "John: The lasagne is already on its way with the delivery person. Unfortunately the driver had an accident and is\n",
      "CPU times: user 4.64 s, sys: 20 ms, total: 4.66 s\n",
      "Wall time: 4.65 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "prompt = tokenizer(text, return_tensors='pt')\n",
    "prompt = {key: value.to(device) for key, value in prompt.items()}\n",
    "out = gpt.generate(**prompt, max_length=prompt['input_ids'].shape[1] + 25, do_sample=False, early_stopping=True)\n",
    "print(tokenizer.decode(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John: Hello, how can I help?\n",
      "what is in the tree?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "horse\n",
      "tree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is in the tree?\n",
      "tree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is in the tree?\n",
      "Tree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is in the tree?\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [47]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m----> 7\u001b[0m     user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     dialogue \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAlberto: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m user_input \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mJohn: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m     text \u001b[38;5;241m=\u001b[39m create_text_from_summary_and_dialogue(summary, dialogue)\n",
      "File \u001b[0;32m~/src/the_chatbot_experiment/.env/lib/python3.8/site-packages/ipykernel/kernelbase.py:1044\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allow_stdin:\n\u001b[1;32m   1041\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(\n\u001b[1;32m   1042\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1043\u001b[0m     )\n\u001b[0;32m-> 1044\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/src/the_chatbot_experiment/.env/lib/python3.8/site-packages/ipykernel/kernelbase.py:1089\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1086\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1087\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1088\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m-> 1089\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m   1090\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1091\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "print(dialogue)\n",
    "sys.stdout.flush()\n",
    "\n",
    "while True:\n",
    "    user_input = input()\n",
    "    dialogue += \"\\nAlberto: \" + user_input + \"\\nJohn: \"\n",
    "    text = create_text_from_summary_and_dialogue(summary, dialogue)\n",
    "    prompt = tokenizer(text, return_tensors='pt')\n",
    "    prompt = {key: value.to(device) for key, value in prompt.items()}\n",
    "    out = gpt.generate(**prompt, max_length=prompt['input_ids'].shape[1] + 25, do_sample=False, early_stopping=True)\n",
    "    out = out[0][prompt['input_ids'].shape[1]:]\n",
    "    answer = tokenizer.decode(out)\n",
    "    answer = answer[:answer.find('\\n')].strip()\n",
    "    print(answer)\n",
    "    dialogue += answer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-7.mnightly-2021-02-12-debian-10-test",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-7:mnightly-2021-02-12-debian-10-test"
  },
  "kernelspec": {
   "display_name": "chatbot3",
   "language": "python",
   "name": "chatbot3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
