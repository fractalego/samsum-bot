{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45879344",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4380648",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings = joblib.load('train_embeddings_logits_only.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98ab1286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['logits_and_indices', 'input_ids'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_embeddings[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898a0fa1",
   "metadata": {},
   "source": [
    "### Training from embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "553939fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import GPT2LMHeadModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5bda74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_small = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13b4e679",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "370453d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_from_summary_and_dialogue(summary, dialogue):\n",
    "    text = f\"\"\"\n",
    "A partial summary of the conversation is:\n",
    "{summary}\n",
    "\n",
    "With the dialogue being:\n",
    "{dialogue}\n",
    "    \"\"\".strip()\n",
    "    \n",
    "    return text.replace('\\r\\n', '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c902d0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = \"\"\"\n",
    "Alberto is a customer. Alberto ordered pizza and lasagne at Dominos one hour ago.\n",
    "Alberto wants to know where his pizza is. Alberto is calling Dominos to know where his orders are.\n",
    "John works at Dominos. John says that his pizza is almost ready. It will be at Alberto's home in 10 minutes. \n",
    "The lasagne is already on its way with the delivery person. Unfortunately the driver had an accident and is now in the hospital.\n",
    "If alberto says \"rabbit\" then John replies \"horse\". If Alberto says \"Tree\" John says \"red\".\n",
    "\"\"\".strip().replace('\\n', ' ')\n",
    "\n",
    "dialogue = \"\"\"\n",
    "Alberto: What happened to my pizza?\n",
    "John: \n",
    "\"\"\".strip()\n",
    "\n",
    "text = create_text_from_summary_and_dialogue(summary, dialogue)\n",
    "\n",
    "prompt = tokenizer(text, return_tensors='pt')\n",
    "prompt = {key: value for key, value in prompt.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f117a243",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "def batchify(data, n):\n",
    "    len_dict = {}\n",
    "    for item in data:\n",
    "        length = item.shape[1]\n",
    "        try:\n",
    "            len_dict[length].append(item)\n",
    "        except:\n",
    "            len_dict[length] = [item]\n",
    "\n",
    "    batch_chunks = []\n",
    "    for k in len_dict.keys():\n",
    "        vectors = len_dict[k]\n",
    "        batch_chunks += chunks(vectors, n)\n",
    "\n",
    "    batches = []\n",
    "    for chunk in batch_chunks:\n",
    "        inputs = torch.stack([item[0] for item in chunk])\n",
    "        batches.append((inputs))\n",
    "\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62d0e120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 0 out of 818\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "val = json.load(open('../data/val.json'))\n",
    "\n",
    "_limit = 1024\n",
    "dev_data = []\n",
    "total_skipped = 0\n",
    "for item in val:\n",
    "    text = create_text_from_summary_and_dialogue(item[\"summary\"], item[\"dialogue\"])\n",
    "    tokens = tokenizer.encode(text, return_tensors='pt')\n",
    "    if tokens.shape[1] > _limit:\n",
    "        tokens = tokens[:, :_limit]\n",
    "    dev_data.append(tokens)\n",
    "    \n",
    "print(f'Skipped {total_skipped} out of {len(val)}')\n",
    "\n",
    "dev_batches = batchify(dev_data, 1)\n",
    "\n",
    "def test(test_model, batches):\n",
    "    test_model.eval()\n",
    "    total_loss = 0.\n",
    "    #for i, batch in tqdm(enumerate(batches), total=len(batches)):\n",
    "    for i, batch in enumerate(batches):\n",
    "        test_model.eval()\n",
    "        inputs = batch\n",
    "        loss = test_model(inputs, labels=inputs)[0]\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "706886b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev loss: 3.37475347081723\n"
     ]
    }
   ],
   "source": [
    "print('Dev loss:', test(gpt_small, dev_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20b4ee54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probability_vector(log_prob_dict, temp):\n",
    "    _vocab_size = 50257\n",
    "    \n",
    "    logits = torch.tensor(log_prob_dict['logits'])\n",
    "    num_tokens = logits.shape[1]\n",
    "    indices = torch.tensor(log_prob_dict['indices'])\n",
    "    vectors = []\n",
    "    \n",
    "    for index_set, logs in zip(indices[0], logits[0]):\n",
    "        v = torch.sparse_coo_tensor([index_set.tolist()], logs, (_vocab_size, )).to_dense().float()\n",
    "        v[v == 0] = torch.tensor(float('-inf'))\n",
    "        vectors.append(v)\n",
    "\n",
    "    vectors = torch.stack(vectors, dim=0)\n",
    "    return F.softmax(vectors / temp, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b900aef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([56, 50257])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_probability_vector(train_embeddings[0]['logits_and_indices'], temp=10).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "776c1e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███████████▌                      | 4999/14732 [3:12:52<6:06:44,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps 5000\n",
      "Dev loss: 3.135260449644989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|███████████████████████           | 9999/14732 [6:32:22<3:06:43,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps 10000\n",
      "Dev loss: 3.147309991141695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 14732/14732 [9:41:40<00:00,  2.37s/it]\n",
      "  2%|▋                                    | 267/14732 [10:18<9:01:42,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps 15000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▌                                 | 268/14732 [17:18<512:33:04, 127.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev loss: 3.1546648274424025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|████████████▏                     | 5267/14732 [3:31:29<6:10:40,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps 20000\n",
      "Dev loss: 3.146964810588249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|████████████▌                     | 5419/14732 [3:44:36<6:25:59,  2.49s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#loss = gpt_small(input_ids, labels=input_ids)[0]\u001b[39;00m\n\u001b[1;32m     26\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(torch\u001b[38;5;241m.\u001b[39mmul(torch\u001b[38;5;241m.\u001b[39mlog(out_p)\u001b[38;5;241m.\u001b[39mflatten(),\n\u001b[1;32m     27\u001b[0m                               label_p\u001b[38;5;241m.\u001b[39mflatten()))\n\u001b[0;32m---> 29\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     32\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/src/the_chatbot_experiment/.env/lib/python3.8/site-packages/torch/_tensor.py:255\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    248\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    249\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    253\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    254\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 255\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/src/the_chatbot_experiment/.env/lib/python3.8/site-packages/torch/autograd/__init__.py:147\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m--> 147\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "lr = 1e-5\n",
    "gamma = 0.9\n",
    "optimizer = torch.optim.Adam(gpt_small.parameters(), lr=lr)\n",
    "scheduler = StepLR(optimizer, step_size=2, gamma=0.9)\n",
    "epochs = 5\n",
    "trace_steps = 5000\n",
    "\n",
    "steps = 0\n",
    "best_loss = 1e6\n",
    "for epoch_num in range(epochs):\n",
    "    temp = 2\n",
    "    random.shuffle(train_embeddings)\n",
    "    \n",
    "    for item in tqdm(train_embeddings):\n",
    "        gpt_small.train()\n",
    "        input_ids = torch.tensor([item['input_ids']])\n",
    "        label_p = get_probability_vector(item['logits_and_indices'], temp=temp)\n",
    "        out_logits = gpt_small.forward(input_ids).logits\n",
    "        out_p = F.softmax(out_logits / temp, dim=-1)\n",
    "        \n",
    "        #loss = gpt_small(input_ids, labels=input_ids)[0]\n",
    "        \n",
    "        loss = - torch.mean(torch.mul(torch.log(out_p).flatten(),\n",
    "                                      label_p.flatten()))\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        steps += 1\n",
    "        \n",
    "        if steps % trace_steps == 0:\n",
    "            print(\"steps\", steps)\n",
    "            print('Dev loss:', test(gpt_small, dev_batches))\n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "                torch.save({'epoch': epoch_num,\n",
    "                            'steps': steps,\n",
    "                            'model_state_dict': gpt_small.state_dict()},\n",
    "                            'save_medium' + str(steps))\n",
    "            \n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a846218",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Dev loss:', test(gpt_small, dev_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5db6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_small.save_pretrained(f'gpt_medium_temp{temp}_lr{lr}_sched{0.5}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130da593",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_small = GPT2LMHeadModel.from_pretrained(\"gpt_medium_temp10_lr1e-05_sched0.9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a47cb8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = gpt_small.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a324ec62",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab179e39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John: Hello, how can I help?\n",
      "how is my pizza doing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it's almost ready\n",
      "when will I receive it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in 10 minutes\n",
      "what about my lasagne\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it's almost ready\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(dialogue)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m----> 9\u001b[0m     user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     dialogue \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAlberto: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m user_input \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mJohn: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     11\u001b[0m     text \u001b[38;5;241m=\u001b[39m create_text_from_summary_and_dialogue(summary, dialogue)\n",
      "File \u001b[0;32m~/src/the_chatbot_experiment/.env/lib/python3.8/site-packages/ipykernel/kernelbase.py:1044\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allow_stdin:\n\u001b[1;32m   1041\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(\n\u001b[1;32m   1042\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1043\u001b[0m     )\n\u001b[0;32m-> 1044\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/src/the_chatbot_experiment/.env/lib/python3.8/site-packages/ipykernel/kernelbase.py:1089\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1086\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1087\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1088\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m-> 1089\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m   1090\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1091\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "dialogue = \"\"\"\n",
    "John: Hello, how can I help?\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "print(dialogue)\n",
    "\n",
    "while True:\n",
    "    user_input = input()\n",
    "    dialogue += \"\\nAlberto: \" + user_input + \"\\nJohn: \"\n",
    "    text = create_text_from_summary_and_dialogue(summary, dialogue)\n",
    "    prompt = tokenizer(text, return_tensors='pt')\n",
    "    prompt = {key: value.to(device) for key, value in prompt.items()}\n",
    "    out = gpt_small.generate(**prompt, max_length=prompt['input_ids'].shape[1] + 25, do_sample=False)\n",
    "    out = out[0][prompt['input_ids'].shape[1]:]\n",
    "    answer = tokenizer.decode(out)\n",
    "    answer = answer[:answer.find('\\n')].strip()\n",
    "    print(answer)\n",
    "    dialogue += answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1634fa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "summary = \"\"\"\n",
    "Alberto is a customer. Alberto ordered pizza and lasagne at Dominos one hour ago.\n",
    "Alberto wants to know where his pizza is. Alberto is calling Dominos to know where his orders are.\n",
    "John works at Dominos. John says that his pizza is almost ready. It will be at Alberto's home in 10 minutes. \n",
    "The lasagne is already on its way with the delivery person. Unfortunately the driver had an accident and is now in the hospital.\n",
    "If alberto says \"rabbit\" then John replies \"horse\". If Alberto says \"Tree\" John says \"red\".\n",
    "\"\"\".strip().replace('\\n', ' ')\n",
    "\n",
    "dialogue = \"\"\"\n",
    "Alberto: What happened to my pizza?\n",
    "John: It's in the delivery man's car.\n",
    "Alberto: And where is the delivery man?\n",
    "\"\"\".strip()\n",
    "\n",
    "text = create_text_from_summary_and_dialogue(summary, dialogue)\n",
    "\n",
    "prompt = tokenizer(text, return_tensors='pt').to('cuda')\n",
    "prompt = {key: value for key, value in prompt.items()}\n",
    "out = gpt_small.generate(**prompt, max_length=prompt['input_ids'].shape[1] + 10, do_sample=False)\n",
    "print(tokenizer.decode(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08573b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_small.save_pretrained('./gptj_small')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccfbfac",
   "metadata": {},
   "source": [
    "#####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a66f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd17cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = torch.sparse_coo_tensor([[1,2,3]], [4,5,6], (1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c190a99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5136f53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.index_select(v, 0, [1,2,3], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb9a4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb802d97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot3",
   "language": "python",
   "name": "chatbot3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
