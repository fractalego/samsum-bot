{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "070cdc3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alce/src/sam_sum_bot/.env/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87254d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "train = json.load(open('../data/train.json'))\n",
    "val = json.load(open('../data/val.json'))\n",
    "test = json.load(open('../data/test.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2478239",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '13728867',\n",
       " 'summary': 'Olivia and Olivier are voting for liberals in this election. ',\n",
       " 'dialogue': 'Olivia: Who are you voting for in this election? \\r\\nOliver: Liberals as always.\\r\\nOlivia: Me too!!\\r\\nOliver: Great'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3222deae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "900b15c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "def batchify(data, n):\n",
    "    len_dict = {}\n",
    "    for item in data:\n",
    "        length = item.shape[1]\n",
    "        try:\n",
    "            len_dict[length].append(item)\n",
    "        except:\n",
    "            len_dict[length] = [item]\n",
    "\n",
    "    batch_chunks = []\n",
    "    for k in len_dict.keys():\n",
    "        vectors = len_dict[k]\n",
    "        batch_chunks += chunks(vectors, n)\n",
    "\n",
    "    batches = []\n",
    "    for chunk in batch_chunks:\n",
    "        inputs = torch.stack([item[0] for item in chunk])\n",
    "        batches.append((inputs))\n",
    "\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfe36c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_from_summary_and_dialogue(summary, dialogue):\n",
    "    text = f\"\"\"\n",
    "A partial summary of the conversation is:\n",
    "{summary}\n",
    "\n",
    "With the dialogue being:\n",
    "{dialogue}\n",
    "    \"\"\".strip()\n",
    "    \n",
    "    return text.replace('\\r\\n', '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b24b6847",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1111 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 0 out of 14732\n"
     ]
    }
   ],
   "source": [
    "_limit = 1024\n",
    "train_data = []\n",
    "total_skipped = 0\n",
    "for item in train:\n",
    "    text = create_text_from_summary_and_dialogue(item[\"summary\"], item[\"dialogue\"])\n",
    "    tokens = tokenizer.encode(text, return_tensors='pt')\n",
    "    if tokens.shape[1] > _limit:\n",
    "        tokens = tokens[:, :_limit]\n",
    "    train_data.append(tokens)\n",
    "    \n",
    "print(f'Skipped {total_skipped} out of {len(train)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "120f3d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 0 out of 818\n"
     ]
    }
   ],
   "source": [
    "_limit = 1024\n",
    "dev_data = []\n",
    "total_skipped = 0\n",
    "for item in val:\n",
    "    text = create_text_from_summary_and_dialogue(item[\"summary\"], item[\"dialogue\"])\n",
    "    tokens = tokenizer.encode(text, return_tensors='pt')\n",
    "    if tokens.shape[1] > _limit:\n",
    "        tokens = tokens[:, :_limit]\n",
    "    dev_data.append(tokens)\n",
    "    \n",
    "print(f'Skipped {total_skipped} out of {len(val)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2bc72f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_model, batches, optimizer, criterion):\n",
    "    train_model.train()\n",
    "    total_loss = 0.\n",
    "    for i, batch in tqdm(enumerate(batches), total=len(batches)):\n",
    "        model.train()\n",
    "        inputs = batch\n",
    "        optimizer.zero_grad()\n",
    "        loss = train_model(inputs.cuda(), labels=inputs.cuda())[0]\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(train_model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(batches)\n",
    "\n",
    "def test(test_model, batches):\n",
    "    test_model.eval()\n",
    "    total_loss = 0.\n",
    "    for i, batch in tqdm(enumerate(batches), total=len(batches)):\n",
    "        test_model.eval()\n",
    "        inputs = batch\n",
    "        loss = test_model(inputs.cuda(), labels=inputs.cuda())[0]\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bb81da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = batchify(train_data, 1)\n",
    "dev_batches = batchify(dev_data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2008ea1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 14732/14732 [14:10<00:00, 17.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 2.396097652887217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 818/818 [00:09<00:00, 90.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev loss: 2.2844983887264374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 14732/14732 [14:15<00:00, 17.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 2.2025161141647516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 818/818 [00:08<00:00, 95.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev loss: 2.2554801596114573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 14732/14732 [14:08<00:00, 17.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Loss: 2.075270324018932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 818/818 [00:08<00:00, 95.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev loss: 2.2475088891598296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|███████████                           | 4274/14732 [04:07<10:04, 17.29it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m     11\u001b[0m     random\u001b[38;5;241m.\u001b[39mshuffle(train_batches)\n\u001b[0;32m---> 12\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch:\u001b[39m\u001b[38;5;124m'\u001b[39m, epoch, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss:\u001b[39m\u001b[38;5;124m'\u001b[39m, loss)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDev loss:\u001b[39m\u001b[38;5;124m'\u001b[39m, test(model, dev_batches))\n",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_model, batches, optimizer, criterion)\u001b[0m\n\u001b[1;32m      9\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     10\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(train_model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(batches)\n",
      "File \u001b[0;32m~/src/sam_sum_bot/.env/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:65\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     64\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/src/sam_sum_bot/.env/lib/python3.8/site-packages/torch/optim/optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/src/sam_sum_bot/.env/lib/python3.8/site-packages/torch/autograd/grad_mode.py:28\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m():\n\u001b[0;32m---> 28\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/src/sam_sum_bot/.env/lib/python3.8/site-packages/torch/optim/adam.py:107\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[38;5;66;03m# record the step after step update\u001b[39;00m\n\u001b[1;32m    105\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 107\u001b[0m     \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m           \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m           \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m           \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m           \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m           \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m           \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m           \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m           \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m           \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m           \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m           \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/src/sam_sum_bot/.env/lib/python3.8/site-packages/torch/optim/_functional.py:87\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m     86\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mmul_(beta1)\u001b[38;5;241m.\u001b[39madd_(grad, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n\u001b[0;32m---> 87\u001b[0m \u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amsgrad:\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;66;03m# Maintains the maximum of all 2nd moment running avg. till now\u001b[39;00m\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39mmaximum(max_exp_avg_sqs[i], exp_avg_sq, out\u001b[38;5;241m=\u001b[39mmax_exp_avg_sqs[i])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-5)\n",
    "\n",
    "model.cuda()\n",
    "\n",
    "random.shuffle(train_batches)\n",
    "scheduler = StepLR(optimizer, step_size=2, gamma=0.8)\n",
    "for epoch in range(10):\n",
    "    random.shuffle(train_batches)\n",
    "    loss = train(model, train_batches, optimizer, criterion)\n",
    "    print('Epoch:', epoch, 'Loss:', loss)\n",
    "    print('Dev loss:', test(model, dev_batches))\n",
    "    torch.save({'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict()},\n",
    "                'save_small' + str(epoch))\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47524dc",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9624a41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "max_probs = 5\n",
    "\n",
    "def generate_answer_and_get_confidence(model, prompt):\n",
    "    tokens = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    _length = 50\n",
    "    tokens_length = tokens.shape[1]\n",
    "    if tokens_length + _length > 1024:\n",
    "        return ''\n",
    "    generated_entropy = 0\n",
    "    while tokens.shape[-1] < tokens_length + _length:\n",
    "        new_token = model(tokens.cuda())\n",
    "        probs = torch.softmax(new_token.logits[:, -1, :], dim=-1)\n",
    "        probs_and_indices = [(p, index) for index, p in enumerate(probs[0].cpu().detach())]\n",
    "        probs_and_indices = sorted(probs_and_indices, key=lambda x: -x[0])\n",
    "        probs = [item[0] for item in probs_and_indices[:max_probs]]\n",
    "        generated_entropy -= np.dot(probs, np.log(probs))\n",
    "        tokens = torch.cat([tokens, torch.tensor([[torch.argmax(new_token.logits[:, -1, :])]])], dim=-1)\n",
    "        last_token = tokens[:, -1]\n",
    "        last_output = tokenizer.decode(last_token, skip_special_tokens=True)\n",
    "        if last_output == '\\n':\n",
    "            break\n",
    "        \n",
    "    print(tokens.shape)\n",
    "    print(tokens_length)\n",
    "    generated_output = tokens[:, tokens_length:]\n",
    "    print(generated_output.shape)\n",
    "    output = tokenizer.decode(generated_output[0], skip_special_tokens=True)\n",
    "    end = output.find('\\n')\n",
    "    return output[:end].strip(), generated_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e8f0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('save_small' + str(2))\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "_ = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e714e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_with_typical_decoding(model, tokenizer, prompt):\n",
    "    tokens = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    _length = 50\n",
    "    tokens_length = tokens.shape[1]\n",
    "    if tokens_length + _length > 1024:\n",
    "        return ''\n",
    "    generated_entropy = 0\n",
    "    while tokens.shape[-1] < tokens_length + _length:\n",
    "        new_tokens = model(tokens.cuda())\n",
    "        normalized = torch.nn.functional.log_softmax(new_tokens.logits, dim=-1)\n",
    "        p = torch.exp(normalized)\n",
    "        entropy = -(normalized * p).nansum(-1, keepdim=True)\n",
    "        shifted_scores = torch.abs(normalized + entropy)\n",
    "        pred_ids = torch.argmin(shifted_scores, dim=-1)\n",
    "        last_token = pred_ids[:, -1].cpu().detach()\n",
    "        tokens = torch.cat([tokens, torch.tensor([[last_token]])], dim=-1)\n",
    "        last_output = tokenizer.decode(last_token, skip_special_tokens=True)\n",
    "        if last_output == '\\n':\n",
    "            break\n",
    "        \n",
    "    generated_output = tokens[:, tokens_length:]\n",
    "    output = tokenizer.decode(generated_output[0], skip_special_tokens=True)\n",
    "    end = output.find('\\n')\n",
    "    return output[:end].replace('A: ', '').strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab74465",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_greedy(model, tokenizer, prompt, max_length=50):\n",
    "    tokens = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    tokens_length = tokens.shape[1]\n",
    "    if tokens_length + max_length > 1024:\n",
    "        return \"\"\n",
    "\n",
    "    while tokens.shape[-1] < tokens_length + max_length:\n",
    "        new_tokens = model(tokens.cuda())\n",
    "        pred_ids = torch.argmax(new_tokens.logits, dim=-1)\n",
    "        last_token = pred_ids[:, -1].cpu().detach()\n",
    "        tokens = torch.cat([tokens, torch.tensor([[last_token]])], dim=-1)\n",
    "        last_output = tokenizer.decode(last_token, skip_special_tokens=True)\n",
    "        if last_output == \"\\n\":\n",
    "            break\n",
    "\n",
    "    generated_output = tokens[:, tokens_length:]\n",
    "    output = tokenizer.decode(generated_output[0], skip_special_tokens=True)\n",
    "    end = output.find(\"\\n\")\n",
    "    return output[:end].replace(\"A: \", \"\").strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908394f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = \"The user asks how the weather is in London. The bot replies 'The chances of raining is 1% today; 0% chances of snow'.\"\n",
    "dialogue = \"\"\"\n",
    "User: Is it going to snow today?\n",
    "Bot: \n",
    "\"\"\".strip()\n",
    "\n",
    "prompt = create_text_from_summary_and_dialogue(summary, dialogue)\n",
    "generate_answer_with_typical_decoding(model, tokenizer, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0514b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = \"\"\"\n",
    "Alberto is a customer. Alberto ordered pizza at Dominos one hour ago.\n",
    "Alberto wants to know where his pizza is. Alberto is calling Dominos to know where his orders are.\n",
    "John works at Dominos. John says that his pizza is almost ready. It will be at Alberto's home in 10 minutes.\n",
    "\"\"\".strip().replace('\\n', ' ')\n",
    "\n",
    "dialogue = \"\"\"\n",
    "Alberto: Hello, where is my pizza?\n",
    "John: \n",
    "\"\"\".strip()\n",
    "\n",
    "prompt = create_text_from_summary_and_dialogue(summary, dialogue)\n",
    "print(generate_answer_greedy(model, tokenizer, prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86a64c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = \"\"\"\n",
    "Alberto is a customer. Alberto ordered pizza at Dominos one hour ago.\n",
    "Alberto wants to know where his pizza is. Alberto is calling Dominos to know where his orders are.\n",
    "John works at Dominos. John says that his pizza is almost ready. It will be at Alberto's home in 10 minutes.\n",
    "\"\"\".strip().replace('\\n', ' ')\n",
    "\n",
    "dialogue = \"\"\"\n",
    "John: Hello, I am John. How can I help you?\n",
    "\"\"\".strip()\n",
    "\n",
    "prompt = create_text_from_summary_and_dialogue(summary, dialogue)\n",
    "print(generate_answer_greedy(model, tokenizer, prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb81669",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dialogue)\n",
    "while True:\n",
    "    user_input = input()\n",
    "    dialogue += \"\\nAlberto: \" + user_input + \"\\nJohn: \"\n",
    "    prompt = create_text_from_summary_and_dialogue(summary, dialogue)\n",
    "    answer = generate_answer_greedy(model, tokenizer, prompt)\n",
    "    print(answer)\n",
    "    dialogue += answer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9748de8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam_sum_bot",
   "language": "python",
   "name": "sam_sum_bot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
