{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "070cdc3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alce/src/sam_sum_bot/.env/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87254d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "train = json.load(open('../data/train.json'))\n",
    "val = json.load(open('../data/val.json'))\n",
    "test = json.load(open('../data/test.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2478239",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '13728867',\n",
       " 'summary': 'Olivia and Olivier are voting for liberals in this election. ',\n",
       " 'dialogue': 'Olivia: Who are you voting for in this election? \\r\\nOliver: Liberals as always.\\r\\nOlivia: Me too!!\\r\\nOliver: Great'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3222deae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2-medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "900b15c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "def batchify(data, n):\n",
    "    len_dict = {}\n",
    "    for item in data:\n",
    "        length = item.shape[1]\n",
    "        try:\n",
    "            len_dict[length].append(item)\n",
    "        except:\n",
    "            len_dict[length] = [item]\n",
    "\n",
    "    batch_chunks = []\n",
    "    for k in len_dict.keys():\n",
    "        vectors = len_dict[k]\n",
    "        batch_chunks += chunks(vectors, n)\n",
    "\n",
    "    batches = []\n",
    "    for chunk in batch_chunks:\n",
    "        inputs = torch.stack([item[0] for item in chunk])\n",
    "        batches.append((inputs))\n",
    "\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfe36c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_from_summary_and_dialogue(summary, dialogue):\n",
    "    text = f\"\"\"\n",
    "A partial summary of the conversation is:\n",
    "{summary}\n",
    "\n",
    "With the dialogue being:\n",
    "{dialogue}\n",
    "    \"\"\".strip()\n",
    "    \n",
    "    return text.replace('\\r\\n', '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b24b6847",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1111 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 0 out of 14732\n"
     ]
    }
   ],
   "source": [
    "_limit = 1024\n",
    "train_data = []\n",
    "total_skipped = 0\n",
    "for item in train:\n",
    "    text = create_text_from_summary_and_dialogue(item[\"summary\"], item[\"dialogue\"])\n",
    "    tokens = tokenizer.encode(text, return_tensors='pt')\n",
    "    if tokens.shape[1] > _limit:\n",
    "        tokens = tokens[:, :_limit]\n",
    "    train_data.append(tokens)\n",
    "    \n",
    "print(f'Skipped {total_skipped} out of {len(train)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "120f3d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 0 out of 818\n"
     ]
    }
   ],
   "source": [
    "_limit = 1024\n",
    "dev_data = []\n",
    "total_skipped = 0\n",
    "for item in val:\n",
    "    text = create_text_from_summary_and_dialogue(item[\"summary\"], item[\"dialogue\"])\n",
    "    tokens = tokenizer.encode(text, return_tensors='pt')\n",
    "    if tokens.shape[1] > _limit:\n",
    "        tokens = tokens[:, :_limit]\n",
    "    dev_data.append(tokens)\n",
    "    \n",
    "print(f'Skipped {total_skipped} out of {len(val)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2bc72f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_model, batches, optimizer, criterion):\n",
    "    train_model.train()\n",
    "    total_loss = 0.\n",
    "    for i, batch in tqdm(enumerate(batches), total=len(batches)):\n",
    "        model.train()\n",
    "        inputs = batch\n",
    "        optimizer.zero_grad()\n",
    "        loss = train_model(inputs, labels=inputs)[0]\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(train_model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(batches)\n",
    "\n",
    "def test(test_model, batches):\n",
    "    test_model.eval()\n",
    "    total_loss = 0.\n",
    "    for i, batch in tqdm(enumerate(batches), total=len(batches)):\n",
    "        test_model.eval()\n",
    "        inputs = batch\n",
    "        loss = test_model(inputs, labels=inputs)[0]\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bb81da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = batchify(train_data, 1)\n",
    "dev_batches = batchify(dev_data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2008ea1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|██▍                                | 1039/14732 [58:25<11:12:41,  2.95s/it]"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-5)\n",
    "\n",
    "random.shuffle(train_batches)\n",
    "scheduler = StepLR(optimizer, step_size=2, gamma=0.8)\n",
    "for epoch in range(10):\n",
    "    random.shuffle(train_batches)\n",
    "    loss = train(model, train_batches, optimizer, criterion)\n",
    "    print('Epoch:', epoch, 'Loss:', loss)\n",
    "    print('Dev loss:', test(model, dev_batches))\n",
    "    torch.save({'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict()},\n",
    "                'save_medium' + str(epoch))\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47524dc",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9624a41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "max_probs = 5\n",
    "\n",
    "def generate_answer_and_get_confidence(model, prompt):\n",
    "    tokens = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    _length = 50\n",
    "    tokens_length = tokens.shape[1]\n",
    "    if tokens_length + _length > 1024:\n",
    "        return ''\n",
    "    generated_entropy = 0\n",
    "    while tokens.shape[-1] < tokens_length + _length:\n",
    "        new_token = model(tokens)\n",
    "        probs = torch.softmax(new_token.logits[:, -1, :], dim=-1)\n",
    "        probs_and_indices = [(p, index) for index, p in enumerate(probs[0].cpu().detach())]\n",
    "        probs_and_indices = sorted(probs_and_indices, key=lambda x: -x[0])\n",
    "        probs = [item[0] for item in probs_and_indices[:max_probs]]\n",
    "        generated_entropy -= np.dot(probs, np.log(probs))\n",
    "        tokens = torch.cat([tokens, torch.tensor([[torch.argmax(new_token.logits[:, -1, :])]])], dim=-1)\n",
    "        last_token = tokens[:, -1]\n",
    "        last_output = tokenizer.decode(last_token, skip_special_tokens=True)\n",
    "        if last_output == '\\n':\n",
    "            break\n",
    "        \n",
    "    print(tokens.shape)\n",
    "    print(tokens_length)\n",
    "    generated_output = tokens[:, tokens_length:]\n",
    "    print(generated_output.shape)\n",
    "    output = tokenizer.decode(generated_output[0], skip_special_tokens=True)\n",
    "    end = output.find('\\n')\n",
    "    return output[:end].strip(), generated_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e8f0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('save_medium' + str(2))\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e714e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_with_typical_decoding(model, tokenizer, prompt):\n",
    "    tokens = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    _length = 50\n",
    "    tokens_length = tokens.shape[1]\n",
    "    if tokens_length + _length > 1024:\n",
    "        return ''\n",
    "    generated_entropy = 0\n",
    "    while tokens.shape[-1] < tokens_length + _length:\n",
    "        new_tokens = model(tokens)\n",
    "        normalized = torch.nn.functional.log_softmax(new_tokens.logits, dim=-1)\n",
    "        p = torch.exp(normalized)\n",
    "        entropy = -(normalized * p).nansum(-1, keepdim=True)\n",
    "        shifted_scores = torch.abs(normalized + entropy)\n",
    "        pred_ids = torch.argmin(shifted_scores, dim=-1)\n",
    "        last_token = pred_ids[:, -1].cpu().detach()\n",
    "        tokens = torch.cat([tokens, torch.tensor([[last_token]])], dim=-1)\n",
    "        last_output = tokenizer.decode(last_token, skip_special_tokens=True)\n",
    "        if last_output == '\\n':\n",
    "            break\n",
    "        \n",
    "    generated_output = tokens[:, tokens_length:]\n",
    "    output = tokenizer.decode(generated_output[0], skip_special_tokens=True)\n",
    "    end = output.find('\\n')\n",
    "    return output[:end].replace('A: ', '').strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab74465",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_greedy(model, tokenizer, prompt, max_length=50):\n",
    "    tokens = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    tokens_length = tokens.shape[1]\n",
    "    if tokens_length + max_length > 1024:\n",
    "        return \"\"\n",
    "\n",
    "    while tokens.shape[-1] < tokens_length + max_length:\n",
    "        new_tokens = model(tokens)\n",
    "        pred_ids = torch.argmax(new_tokens.logits, dim=-1)\n",
    "        last_token = pred_ids[:, -1].cpu().detach()\n",
    "        tokens = torch.cat([tokens, torch.tensor([[last_token]])], dim=-1)\n",
    "        last_output = tokenizer.decode(last_token, skip_special_tokens=True)\n",
    "        if last_output == \"\\n\":\n",
    "            break\n",
    "\n",
    "    generated_output = tokens[:, tokens_length:]\n",
    "    output = tokenizer.decode(generated_output[0], skip_special_tokens=True)\n",
    "    end = output.find(\"\\n\")\n",
    "    return output[:end].replace(\"A: \", \"\").strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908394f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = \"The user asks how the weather is in London. The bot replies 'The chances of raining is 1% today; 0% chances of snow'.\"\n",
    "dialogue = \"\"\"\n",
    "User: Is it going to snow today?\n",
    "Bot: \n",
    "\"\"\".strip()\n",
    "\n",
    "prompt = create_text_from_summary_and_dialogue(summary, dialogue)\n",
    "generate_answer_with_typical_decoding(model, tokenizer, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0514b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = \"\"\"\n",
    "Alberto is a customer. Alberto ordered pizza at Dominos one hour ago.\n",
    "Alberto wants to know where his pizza is. Alberto is calling Dominos to know where his orders are.\n",
    "John works at Dominos. John says that his pizza is almost ready. It will be at Alberto's home in 10 minutes.\n",
    "\"\"\".strip().replace('\\n', ' ')\n",
    "\n",
    "dialogue = \"\"\"\n",
    "Alberto: Hello, where is my pizza?\n",
    "John: \n",
    "\"\"\".strip()\n",
    "\n",
    "prompt = create_text_from_summary_and_dialogue(summary, dialogue)\n",
    "print(generate_answer_greedy(model, tokenizer, prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86a64c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = \"\"\"\n",
    "Alberto is a customer. Alberto ordered pizza at Dominos one hour ago.\n",
    "Alberto wants to know where his pizza is. Alberto is calling Dominos to know where his orders are.\n",
    "John works at Dominos. John says that his pizza is almost ready. It will be at Alberto's home in 10 minutes.\n",
    "\"\"\".strip().replace('\\n', ' ')\n",
    "\n",
    "dialogue = \"\"\"\n",
    "John: Hello, I am John. How can I help you?\n",
    "\"\"\".strip()\n",
    "\n",
    "prompt = create_text_from_summary_and_dialogue(summary, dialogue)\n",
    "print(generate_answer_greedy(model, tokenizer, prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb81669",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dialogue)\n",
    "while True:\n",
    "    user_input = input()\n",
    "    dialogue += \"\\nAlberto: \" + user_input + \"\\nJohn: \"\n",
    "    prompt = create_text_from_summary_and_dialogue(summary, dialogue)\n",
    "    answer = generate_answer_greedy(model, tokenizer, prompt)\n",
    "    print(answer)\n",
    "    dialogue += answer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9748de8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam_sum_bot",
   "language": "python",
   "name": "sam_sum_bot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
